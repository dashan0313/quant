{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>pre_close</th>\n",
       "      <th>change</th>\n",
       "      <th>pct_chg</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trade_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-01-02</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>20030102</td>\n",
       "      <td>10.35</td>\n",
       "      <td>10.35</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.03</td>\n",
       "      <td>10.49</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>64848.85</td>\n",
       "      <td>65629.6488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-03</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>20030103</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.10</td>\n",
       "      <td>9.91</td>\n",
       "      <td>10.02</td>\n",
       "      <td>10.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>49594.69</td>\n",
       "      <td>49522.8547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-06</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>20030106</td>\n",
       "      <td>10.03</td>\n",
       "      <td>10.12</td>\n",
       "      <td>9.91</td>\n",
       "      <td>10.06</td>\n",
       "      <td>10.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.40</td>\n",
       "      <td>36553.07</td>\n",
       "      <td>36626.6662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-07</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>20030107</td>\n",
       "      <td>10.06</td>\n",
       "      <td>10.15</td>\n",
       "      <td>10.01</td>\n",
       "      <td>10.04</td>\n",
       "      <td>10.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>27149.59</td>\n",
       "      <td>27361.4667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-08</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>20030108</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.74</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.53</td>\n",
       "      <td>10.04</td>\n",
       "      <td>0.49</td>\n",
       "      <td>4.88</td>\n",
       "      <td>80421.70</td>\n",
       "      <td>83768.8306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ts_code trade_date   open   high    low  close  pre_close  \\\n",
       "trade_date                                                                \n",
       "2003-01-02  000001.SZ   20030102  10.35  10.35   9.99  10.03      10.49   \n",
       "2003-01-03  000001.SZ   20030103  10.00  10.10   9.91  10.02      10.03   \n",
       "2003-01-06  000001.SZ   20030106  10.03  10.12   9.91  10.06      10.02   \n",
       "2003-01-07  000001.SZ   20030107  10.06  10.15  10.01  10.04      10.06   \n",
       "2003-01-08  000001.SZ   20030108  10.00  10.74  10.00  10.53      10.04   \n",
       "\n",
       "            change  pct_chg       vol      amount  \n",
       "trade_date                                         \n",
       "2003-01-02   -0.46    -4.39  64848.85  65629.6488  \n",
       "2003-01-03   -0.01    -0.10  49594.69  49522.8547  \n",
       "2003-01-06    0.04     0.40  36553.07  36626.6662  \n",
       "2003-01-07   -0.02    -0.20  27149.59  27361.4667  \n",
       "2003-01-08    0.49     4.88  80421.70  83768.8306  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tushare as ts\n",
    "\n",
    "token = 'c5cba6dd1dc9c4c3578cdd046f54dfc4119fef2aabf7f998ed9a5192'\n",
    "ts.set_token(token)\n",
    "pro = ts.pro_api(token)\n",
    "\n",
    "df=pro.daily(ts_code='000001.SZ', start_date='20030101',\n",
    "               end_date='20190106')\n",
    "\n",
    "df.index = pd.to_datetime(df.trade_date)\n",
    "df.sort_index(ascending=True,inplace = True)  #####\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3735, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jameson/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/jameson/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1783: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "df_s = df[['close','open']]\n",
    "df_s.loc[:,'gap'] = df_s.close - df_s.open\n",
    "df_s.loc[:,'mean5'] = df_s.close.rolling(5).mean()\n",
    "df_s = df_s.drop('open',axis=1).reset_index(drop=True)\n",
    "df_s = df_s.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>gap</th>\n",
       "      <th>mean5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>10.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.69</td>\n",
       "      <td>0.14</td>\n",
       "      <td>10.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.53</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>10.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.40</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>10.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.44</td>\n",
       "      <td>1.03</td>\n",
       "      <td>10.718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   close   gap   mean5\n",
       "0  10.53  0.53  10.136\n",
       "1  10.69  0.14  10.268\n",
       "2  10.53 -0.12  10.370\n",
       "3  10.40 -0.08  10.438\n",
       "4  11.44  1.03  10.718"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3731, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tvs(data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ts_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, target_col, categorical_cols,numerical_cols,seq_length,prediction_window):\n",
    "        self.data = data\n",
    "        self.target_col = target_col\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.numerical_cols = list(set(data.columns) - set(categorical_cols) - set(target_col))\n",
    "        self.seq_length = seq_length\n",
    "        self.prediction_window = prediction_window\n",
    "        self.preprocessor = None\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index: index+self.seq_length].values\n",
    "        y = self.data[self.target_col].values[index+self.seq_length : index+self.seq_length+self.prediction_window]\n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "    def split_tvs(self, X ,y, validation_size, test_size,X_mean,X_std,y_mean,y_std):\n",
    "        '''split my dataset into training_set, validation_set and prediction_set'''\n",
    "        \n",
    "        def split_single(X,y,start,end):\n",
    "            features, target = [],[]\n",
    "            \n",
    "            for i in range(start,end):\n",
    "                features.append(torch.FloatTensor((X[i: i+self.seq_length, :] - X_mean)/X_std).unsqueeze(0))\n",
    "                target.append(torch.FloatTensor((y[i + self.seq_length: i+self.seq_length+self.prediction_window]) - y_mean/y_std).unsqueeze(0))\n",
    "                \n",
    "            features_var = torch.cat(features)\n",
    "            target_var = torch.cat(target)\n",
    "        \n",
    "            return (features_var, target_var)\n",
    "            \n",
    "        train_X, train_y = split_single(X, y, \n",
    "                                        0,\n",
    "                                       len(self.data)-self.seq_length- prediction_size-validation_size-self.prediction_window)\n",
    "        val_X, val_y = split_single(X,y, \n",
    "                                    len(self.data)-self.seq_length- prediction_size-validation_size-self.prediction_window, \n",
    "                                    len(self.data)-self.seq_length- prediction_size-self.prediction_window)\n",
    "        test_X, test_y = split_single(X, y, \n",
    "                                      len(self.data) - self.seq_length - prediction_size -self.prediction_window, \n",
    "                                      len(self.data) - self.seq_length - self.prediction_window)\n",
    "        return (train_X,train_y), (val_X,val_y),(test_X, test_y)\n",
    "    \n",
    "    \n",
    "    def preprocess_data(self,validation_size,test_size):\n",
    "        '''Preprocessing function'''\n",
    "        #\n",
    "        X = self.data[numerical_cols + categorical_cols].values\n",
    "        y = self.data[target_col].values\n",
    "        \n",
    "        X_mean = X[:(-validation_size-test_size)].mean(axis=0)\n",
    "        X_std = X[:(-validation_size-test_size)].std(axis=0)\n",
    "        \n",
    "        y_mean = y[:(-validation_size-test_size)].mean()\n",
    "        y_std = y[:(-validation_size-test_size)].std()\n",
    "        \n",
    "        train_set, val_set, test_set = self.split_tvs(X ,y, validation_size, test_size, X_mean,X_std,y_mean,y_std)\n",
    "\n",
    "        return (train_set, val_set, test_set)\n",
    "    \n",
    "    \n",
    "    def get_loader(self, validation_size,test_size, batch_size: int, drop_last =True):\n",
    "        \n",
    "        train_set, val_set, test_set = self.preprocess_data(validation_size, test_size)\n",
    "        \n",
    "        train_iter = DataLoader(TensorDataset(*train_set), batch_size = batch_size, shuffle = True, drop_last= drop_last)\n",
    "        val_iter = DataLoader(TensorDataset(*val_set), batch_size = batch_size, shuffle = True, drop_last= drop_last)\n",
    "        #test_iter = DataLoader(TensorDataset(*test_set), batch_size = batch_size, shuffle = False, drop_last= drop_last)\n",
    "        \n",
    "        return train_iter, val_iter, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'close'\n",
    "categorical_cols = []\n",
    "numerical_cols = ['close','gap','mean5']\n",
    "#numerical_cols = ['close']\n",
    "seq_length = 30\n",
    "prediction_window = 7\n",
    "\n",
    "my_dataset = ts_dataset(df_s,target_col,categorical_cols, numerical_cols,seq_length,prediction_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = my_dataset.get_loader(300,100,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 7])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_loader).next()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30, 3])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_loader).next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "        input_size = 3,\n",
    "        hidden_size = 128,\n",
    "        num_layers= 1,\n",
    "        batch_first = True)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "        nn.Linear(128,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.lstm(x, None)   # 'None' stands for use all zeros as hidden state input\n",
    "        out = self.out(r_out[:, -7:, :])                     # use the last day as output\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "EPOCH = 100\n",
    "EARLY_STOP = True\n",
    "EARLY_STOP_STEP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 0   step : 0 train_loss :  177.6470\n",
      "epoch : 0   step : 10 train_loss :  233.8739\n",
      "epoch : 0   step : 20 train_loss :  245.5990\n",
      "epoch : 0  val_loss : 83.7334\n",
      "new model saved at epoch 0 with val_loss 83.73338317871094\n",
      "epoch : 0  val_loss : 89.9347\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 1   step : 0 train_loss :  197.8448\n",
      "epoch : 1   step : 10 train_loss :  200.1635\n",
      "epoch : 1   step : 20 train_loss :  229.1023\n",
      "epoch : 1  val_loss : 88.2541\n",
      "epoch : 1  val_loss : 82.6610\n",
      "new model saved at epoch 1 with val_loss 82.66101837158203\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 2   step : 0 train_loss :  188.0148\n",
      "epoch : 2   step : 10 train_loss :  216.3756\n",
      "epoch : 2   step : 20 train_loss :  156.6379\n",
      "epoch : 2  val_loss : 84.9589\n",
      "epoch : 2  val_loss : 79.4133\n",
      "new model saved at epoch 2 with val_loss 79.41325378417969\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 3   step : 0 train_loss :  194.4279\n",
      "epoch : 3   step : 10 train_loss :  177.4911\n",
      "epoch : 3   step : 20 train_loss :  190.7165\n",
      "epoch : 3  val_loss : 70.9053\n",
      "new model saved at epoch 3 with val_loss 70.90528106689453\n",
      "epoch : 3  val_loss : 71.9150\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 4   step : 0 train_loss :  165.8723\n",
      "epoch : 4   step : 10 train_loss :  144.4597\n",
      "epoch : 4   step : 20 train_loss :  106.8020\n",
      "epoch : 4  val_loss : 30.3129\n",
      "new model saved at epoch 4 with val_loss 30.312910079956055\n",
      "epoch : 4  val_loss : 26.3846\n",
      "new model saved at epoch 4 with val_loss 26.384607315063477\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 5   step : 0 train_loss :  123.8597\n",
      "epoch : 5   step : 10 train_loss :  102.5672\n",
      "epoch : 5   step : 20 train_loss :  117.3878\n",
      "epoch : 5  val_loss : 16.1210\n",
      "new model saved at epoch 5 with val_loss 16.121002197265625\n",
      "epoch : 5  val_loss : 17.7313\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 6   step : 0 train_loss :  115.6669\n",
      "epoch : 6   step : 10 train_loss :  84.1567\n",
      "epoch : 6   step : 20 train_loss :  101.8679\n",
      "epoch : 6  val_loss : 10.9740\n",
      "new model saved at epoch 6 with val_loss 10.973970413208008\n",
      "epoch : 6  val_loss : 11.4558\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 7   step : 0 train_loss :  72.2407\n",
      "epoch : 7   step : 10 train_loss :  68.0564\n",
      "epoch : 7   step : 20 train_loss :  137.6709\n",
      "epoch : 7  val_loss : 6.7901\n",
      "new model saved at epoch 7 with val_loss 6.790104389190674\n",
      "epoch : 7  val_loss : 8.2715\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 8   step : 0 train_loss :  81.8325\n",
      "epoch : 8   step : 10 train_loss :  115.2727\n",
      "epoch : 8   step : 20 train_loss :  73.7739\n",
      "epoch : 8  val_loss : 5.5709\n",
      "new model saved at epoch 8 with val_loss 5.570851802825928\n",
      "epoch : 8  val_loss : 5.1395\n",
      "new model saved at epoch 8 with val_loss 5.139487266540527\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 9   step : 0 train_loss :  83.4041\n",
      "epoch : 9   step : 10 train_loss :  75.9199\n",
      "epoch : 9   step : 20 train_loss :  59.3987\n",
      "epoch : 9  val_loss : 3.5686\n",
      "new model saved at epoch 9 with val_loss 3.568552017211914\n",
      "epoch : 9  val_loss : 4.6113\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 10   step : 0 train_loss :  110.6694\n",
      "epoch : 10   step : 10 train_loss :  80.7281\n",
      "epoch : 10   step : 20 train_loss :  69.0535\n",
      "epoch : 10  val_loss : 3.2542\n",
      "new model saved at epoch 10 with val_loss 3.2541661262512207\n",
      "epoch : 10  val_loss : 3.2643\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 11   step : 0 train_loss :  56.5128\n",
      "epoch : 11   step : 10 train_loss :  73.8092\n",
      "epoch : 11   step : 20 train_loss :  70.9664\n",
      "epoch : 11  val_loss : 2.5728\n",
      "new model saved at epoch 11 with val_loss 2.5727789402008057\n",
      "epoch : 11  val_loss : 2.6872\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 12   step : 0 train_loss :  82.8653\n",
      "epoch : 12   step : 10 train_loss :  74.2992\n",
      "epoch : 12   step : 20 train_loss :  82.7145\n",
      "epoch : 12  val_loss : 2.8321\n",
      "epoch : 12  val_loss : 2.4203\n",
      "new model saved at epoch 12 with val_loss 2.4203202724456787\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 13   step : 0 train_loss :  42.0140\n",
      "epoch : 13   step : 10 train_loss :  66.9270\n",
      "epoch : 13   step : 20 train_loss :  70.1689\n",
      "epoch : 13  val_loss : 2.5613\n",
      "epoch : 13  val_loss : 2.6335\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 14   step : 0 train_loss :  81.0503\n",
      "epoch : 14   step : 10 train_loss :  78.1197\n",
      "epoch : 14   step : 20 train_loss :  69.5489\n",
      "epoch : 14  val_loss : 2.4671\n",
      "epoch : 14  val_loss : 2.8160\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 15   step : 0 train_loss :  47.0942\n",
      "epoch : 15   step : 10 train_loss :  60.5660\n",
      "epoch : 15   step : 20 train_loss :  71.1696\n",
      "epoch : 15  val_loss : 2.8587\n",
      "epoch : 15  val_loss : 3.2399\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 16   step : 0 train_loss :  68.2673\n",
      "epoch : 16   step : 10 train_loss :  51.2819\n",
      "epoch : 16   step : 20 train_loss :  56.3619\n",
      "epoch : 16  val_loss : 3.2268\n",
      "epoch : 16  val_loss : 3.2363\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 17   step : 0 train_loss :  37.9652\n",
      "epoch : 17   step : 10 train_loss :  61.9797\n",
      "epoch : 17   step : 20 train_loss :  66.2065\n",
      "epoch : 17  val_loss : 3.6403\n",
      "epoch : 17  val_loss : 3.2324\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 18   step : 0 train_loss :  48.9317\n",
      "epoch : 18   step : 10 train_loss :  82.5665\n",
      "epoch : 18   step : 20 train_loss :  56.4876\n",
      "epoch : 18  val_loss : 4.1670\n",
      "epoch : 18  val_loss : 3.9100\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 19   step : 0 train_loss :  56.8335\n",
      "epoch : 19   step : 10 train_loss :  53.9030\n",
      "epoch : 19   step : 20 train_loss :  70.5136\n",
      "epoch : 19  val_loss : 3.8858\n",
      "epoch : 19  val_loss : 4.8775\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 20   step : 0 train_loss :  61.5704\n",
      "epoch : 20   step : 10 train_loss :  72.5718\n",
      "epoch : 20   step : 20 train_loss :  53.7585\n",
      "epoch : 20  val_loss : 4.2187\n",
      "epoch : 20  val_loss : 4.7847\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 21   step : 0 train_loss :  48.8764\n",
      "epoch : 21   step : 10 train_loss :  51.3674\n",
      "epoch : 21   step : 20 train_loss :  54.2147\n",
      "epoch : 21  val_loss : 3.9102\n",
      "epoch : 21  val_loss : 4.6923\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 22   step : 0 train_loss :  35.5011\n",
      "epoch : 22   step : 10 train_loss :  58.3937\n",
      "epoch : 22   step : 20 train_loss :  50.2417\n",
      "epoch : 22  val_loss : 2.8498\n",
      "epoch : 22  val_loss : 2.9898\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 23   step : 0 train_loss :  43.6984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 23   step : 10 train_loss :  61.3209\n",
      "epoch : 23   step : 20 train_loss :  36.2653\n",
      "epoch : 23  val_loss : 2.2537\n",
      "new model saved at epoch 23 with val_loss 2.2536532878875732\n",
      "epoch : 23  val_loss : 1.8277\n",
      "new model saved at epoch 23 with val_loss 1.8277130126953125\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 24   step : 0 train_loss :  52.0057\n",
      "epoch : 24   step : 10 train_loss :  45.7360\n",
      "epoch : 24   step : 20 train_loss :  31.1103\n",
      "epoch : 24  val_loss : 1.4231\n",
      "new model saved at epoch 24 with val_loss 1.4230583906173706\n",
      "epoch : 24  val_loss : 1.3253\n",
      "new model saved at epoch 24 with val_loss 1.3253310918807983\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 25   step : 0 train_loss :  50.8748\n",
      "epoch : 25   step : 10 train_loss :  40.0238\n",
      "epoch : 25   step : 20 train_loss :  43.3733\n",
      "epoch : 25  val_loss : 1.3902\n",
      "epoch : 25  val_loss : 1.1100\n",
      "new model saved at epoch 25 with val_loss 1.1100202798843384\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 26   step : 0 train_loss :  24.0701\n",
      "epoch : 26   step : 10 train_loss :  42.7160\n",
      "epoch : 26   step : 20 train_loss :  30.3836\n",
      "epoch : 26  val_loss : 0.7021\n",
      "new model saved at epoch 26 with val_loss 0.7020917534828186\n",
      "epoch : 26  val_loss : 0.8405\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 27   step : 0 train_loss :  49.5662\n",
      "epoch : 27   step : 10 train_loss :  36.6820\n",
      "epoch : 27   step : 20 train_loss :  33.4173\n",
      "epoch : 27  val_loss : 1.0318\n",
      "epoch : 27  val_loss : 0.9791\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 28   step : 0 train_loss :  63.8892\n",
      "epoch : 28   step : 10 train_loss :  27.5237\n",
      "epoch : 28   step : 20 train_loss :  37.3491\n",
      "epoch : 28  val_loss : 1.1697\n",
      "epoch : 28  val_loss : 1.0448\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 29   step : 0 train_loss :  34.0335\n",
      "epoch : 29   step : 10 train_loss :  35.7273\n",
      "epoch : 29   step : 20 train_loss :  21.7183\n",
      "epoch : 29  val_loss : 1.0805\n",
      "epoch : 29  val_loss : 0.7901\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 30   step : 0 train_loss :  22.0614\n",
      "epoch : 30   step : 10 train_loss :  32.8658\n",
      "epoch : 30   step : 20 train_loss :  44.7379\n",
      "epoch : 30  val_loss : 0.9022\n",
      "epoch : 30  val_loss : 1.0605\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 31   step : 0 train_loss :  48.9581\n",
      "epoch : 31   step : 10 train_loss :  40.8449\n",
      "epoch : 31   step : 20 train_loss :  26.3349\n",
      "epoch : 31  val_loss : 0.8107\n",
      "epoch : 31  val_loss : 1.1158\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 32   step : 0 train_loss :  44.1426\n",
      "epoch : 32   step : 10 train_loss :  23.1717\n",
      "epoch : 32   step : 20 train_loss :  31.7588\n",
      "epoch : 32  val_loss : 0.9507\n",
      "epoch : 32  val_loss : 0.8502\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 33   step : 0 train_loss :  28.5091\n",
      "epoch : 33   step : 10 train_loss :  39.8654\n",
      "epoch : 33   step : 20 train_loss :  27.3164\n",
      "epoch : 33  val_loss : 0.8439\n",
      "epoch : 33  val_loss : 0.8499\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 34   step : 0 train_loss :  28.1876\n",
      "epoch : 34   step : 10 train_loss :  32.5147\n",
      "epoch : 34   step : 20 train_loss :  33.9960\n",
      "epoch : 34  val_loss : 0.8721\n",
      "epoch : 34  val_loss : 0.8244\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 35   step : 0 train_loss :  25.2640\n",
      "epoch : 35   step : 10 train_loss :  37.5581\n",
      "epoch : 35   step : 20 train_loss :  20.7768\n",
      "epoch : 35  val_loss : 0.8335\n",
      "epoch : 35  val_loss : 0.7945\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "epoch : 36   step : 0 train_loss :  33.9121\n",
      "epoch : 36   step : 10 train_loss :  31.2961\n",
      "epoch : 36   step : 20 train_loss :  34.2826\n",
      "epoch : 36  val_loss : 0.8692\n",
      "epoch : 36  val_loss : 0.8172\n",
      "***************warning: early stop**************\n"
     ]
    }
   ],
   "source": [
    "rnn = LSTM()\n",
    "\n",
    "# put model into cuda\n",
    "if torch.cuda.is_available():\n",
    "    rnn = rnn.cuda\n",
    "\n",
    "############optimizer and loss function###############\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = LR)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "best_loss = np.inf\n",
    "##################################################\n",
    "\n",
    "if not os.path.exists('weights'):\n",
    "    os.mkdir('weights')\n",
    "\n",
    "early_stop_now = 0    \n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print(\"--\"*60)\n",
    "    step = 0\n",
    "    # trainning\n",
    "    for tx, ty in train_loader:\n",
    "    \n",
    "        if torch.cuda.is_available():\n",
    "            tx = tx.cuda()\n",
    "            ty = ty.cuda()\n",
    "            \n",
    "            ############### \n",
    "        output = rnn(tx)\n",
    "        loss = loss_func(torch.squeeze(output), ty)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('epoch : %d  ' %epoch, 'step : %d' %step, 'train_loss :  %.4f' % loss.cpu().item())\n",
    "            \n",
    "        step += 1\n",
    "    \n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        for tx, ty in val_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                tx = tx.cuda()\n",
    "                ty = ty.cuda()\n",
    "                \n",
    "            output = rnn(tx) #    rnn(torch.unsqueeze(tx, dim = 2))\n",
    "            #unsqueeze: returns a new tensor with a dimension of size one inserted at the specified position\n",
    "            loss = loss_func(torch.squeeze(output), ty)  #squeeze: retures a tensor with all the dim of input of sieze 1 removed\n",
    "            \n",
    "            print('epoch : %d ' % epoch, 'val_loss : %.4f' % loss.cpu().item())\n",
    "            \n",
    "            if loss.cpu().item() < best_loss:\n",
    "                early_stop_now = 0\n",
    "                best_loss = loss.cpu().item()\n",
    "                torch.save(rnn.state_dict(), 'weights/rnn.pkl')\n",
    "                print('new model saved at epoch {} with val_loss {}'.format(epoch, best_loss))\n",
    "            else:\n",
    "                early_stop_now += 1\n",
    "                \n",
    "    if EARLY_STOP and early_stop_now > EARLY_STOP_STEP:\n",
    "        print('***************warning: early stop**************')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
